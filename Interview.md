### 1. 讲一下随机森林，XGboost和GBDT
##### 1) 随机森林和GBDT的相同点
- 都是由多棵树组合而成的
- 最终结果都是由多棵树共同决定的

##### 2) 随机森林和GBDT的不同点
- 随机森林是属于bagging集成学习, GBDT是属于boosting集成学习
- 随机森林是并行迭代的， GBDT是串行迭代的
- 随机森林是通过减少方差来提升性能，GBDT是通过减少偏差来提升性能的
- 随机森林的结果是由多数表决的，GBDT是由多棵树累加得到的
- 随机森林对异常值不敏感，GBDT对异常值比较敏感
- 随机森林不需要进行归一化处理，GBDT需要进行归一化处理

##### 3) 分类树和回归树的区别
- 分类树是由信息增益或信息增益比或gini系数来划分节点，每个节点样本的类别情况作为测试样本的类别
- 回归树是由均方误差来划分节点的，每个节点样本的均值作为测试样本的回归预测值

##### 4) GBDT和Adaboost的核心思想
- GBDT： 基于上轮强学习器的误差，拟合一颗CART回归树，使得本轮的损失函数变得最小
- Adaboost：利用前一轮弱学习器的误差率，更新样本权重系数，然后生成当轮的弱学习器
- GBDT和Adaboost重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的
权重来定位模型的不足而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。

##### 5) GBDT和XGboost的不同
- 1. 算法本身的不同：
- XGboost的弱学习器可以选择多种模型，GBDT只能选择CART回归树
- XGboost的loss函数有额外的正则项
- XGboost的优化算法是二阶泰勒，GBDT是梯度，也就是一阶泰勒
- 2. 算法的运行效率：
boosting算法的弱学习器不能进行并行迭代，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGboost对其进行了大部分优化。对于不同的特征的特征划分点，
XGboost分别在不同的线程中并行选择分裂的最大增益。

同时，对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量。
通过设置合理的分块的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。另外，通过将分块进行压缩（block compressoin）
并存储到硬盘上，并且通过将分块分区到多个硬盘上实现了更大的IO。

- 3. 算法的运健壮性
正则化项提高算法的泛化能力外，XGBoost还对特征的缺失值做了处理。







特征重要性：在集成学习中，模型可以获得特征重要性，用来筛选特征，使模型鲁棒性更好

1.RF

思想：判断每个特征在随机森林中每个树上的贡献，最后取平均值，然后比一比各特征贡献的大小，主要采用基尼指数、基于袋外数据

（1）基于基尼指数



（2）基于袋外数据

思想：用OOB（袋外数据做预测）：RF在重采样建立决策树时，会有一些样本没有被选中，利用这些样本去做交叉验证，或者不用做交叉验证，直接用oob_score去对模型性能做评估

方法：对每棵决策树，用OOB计算袋外数据误差，errOOB1；然后随机对OOB所有样本的特征加入噪声干扰，再次计算袋外数据误差，errOOB2；假设有N棵树，特征i的重要性为【sum（errOOB2-errOOB1）】/N

如果加入随机噪声后，袋外数据准确率大幅下降，说明这个特征对预测结果有很大的影响，说明他的重要性比较高

【噪声干扰】：随机改变第j列，其他列不变，对第j列进行随机的上下替换，E1-E2来刻画特征j的重要性

而该方法中涉及到的对数据进行打乱的方法通常有两种：

1）是使用uniform或者gaussian抽取随机值替换原特征；

2）是通过permutation的方式将原来的所有N个样本的第 i 个特征值重新打乱分布（相当于重新洗牌）



2.XGB

（1） weight：使用特征在所有树中作为划分属性的次数

（2）gain：使用特征在作为划分属性时loss平均的降低量

（3）cover：使用特征作为划分属性时对样本的覆盖度


### 2. word2vec和fasttext区别
- 模型目的不同：Fasttext是文本分类器，word2vec是为了训练词向量
- fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。
- fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词
- fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。
- fastText除了词袋模型还添加了N-gram特征，但是需要过滤掉低频的N-gram  
1. 都可以无监督学习词向量， fastText训练词向量时会考虑subword  
2. fastText还可以进行有监督学习进行文本分类，其主要特点

结构与CBOW类似，但学习目标是人工标注的分类结果；
采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；
引入N-gram，考虑词序特征；
引入subword来处理长词，处理未登陆词问题；

- https://zhuanlan.zhihu.com/p/57549800
- https://zhuanlan.zhihu.com/p/56382372

### 3. lstm已经能做序列标注，为什么还需要crf？
- 它们的预测机理是不同的。LSTM是用神经网络的超强非线性拟合能力来预测。而CRF是用全局范围内统计归一化的条件状态转移概率矩阵来预测。
- lstm能学习到观测序列的特征（观测序列之间的关系），而crf能学习到状态序列的特征（状态序列之间的关系）。

- https://zhuanlan.zhihu.com/p/44042528

### 4. attention机制的原理？
- 原理就是在decoding阶段对input中的信息赋予不同的权重。在nlp中就是针对sequence的每个time step input.

### 5. 介绍一下ELMO
- ELMO的本质是根据上下文对word embedding进行动态调整。 
- ELMO包括两个阶段：一是基于language model的预训练。二是在做下游任务时，从预训练模型中提取对应单词网络各层的word embedding作为新特征补充到下游任务重
- 语言模型训练的任务目标是根据wi的上下文来预测单词wi
- 第一层是单词特征，第二层是句法特征，第三层是语义特征

- https://zhuanlan.zhihu.com/p/63115885

### 5. nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert
##### 1）文本表示
- 基于one-hot、tf-idf、textrank等的bag-of-words；
- 主题模型：LSA（SVD）、pLSA、LDA；
- 基于词向量的固定表征：word2vec、fastText、glove
- 基于词向量的动态表征：elmo、GPT、bert

##### 2）怎么从语言模型理解词向量？怎么理解分布式假设？
- 分布式假设：相同上下文的词具有相似含义
- word2vec， fasttext虽然本质是语言模型，但目标并不是语言模型本身，而是词向量。glove则是基于全局语料库，并结合上下文语境构建词向量，结合了word2vec和LSA（SVM）的优点。

##### 3） diffrence between word2vec and glove
- word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。
- word2vec是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数[公式]。
- word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。
- 总体来看，glove可以被看作是更换了目标函数和权重函数的全局word2vec。
