### 1. 预训练语言模型专题的第3篇(参考reference 7)
上述文章进行了很多完备的实验，来提出和验证了在文本分类任务上，BERT的较好的finetune策略及其效果。文章验证了 :  1) 对于文本分类，BERT的最高层的效果最好 2) 使用合适的层间学习率下降策略，BERT能够克服灾难性遗忘的问题 3) 任务相关或领域内相关数据的语言模型finetune可以大大提升效果 4) 相关的多任务学习也对特定任务有提升效果 5) BERT只需少量的特定数据就可以预训练以提升任务。文章对我们训练BERT模型有很好的指导意义，以后再也不用担心炸炉啦！






### Reference
1. 对BERT的pretraining改进的几篇文章 http://fancyerii.github.io/2019/08/02/bert-pretrain-imp/
2. 中文全词覆盖（Whole Word Masking）BERT的预训练模型 https://www.ctolib.com/ymcui-Chinese-BERT-wwm.html
3. 从0到1开始训练一个bert语言模型 https://blog.csdn.net/luoyexuge/article/details/85001859
4. BERT模型从训练到部署 https://blog.csdn.net/xmxoxo/article/details/89315370
5. Bert 预训练小结 https://zhuanlan.zhihu.com/p/74090249
6. google bert模型  https://github.com/google-research/albert
7. 预训练语言模型专题的第15篇。 https://www.zhihu.com/people/yangddd-39
