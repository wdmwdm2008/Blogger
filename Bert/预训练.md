### Reference
1. 对BERT的pretraining改进的几篇文章 http://fancyerii.github.io/2019/08/02/bert-pretrain-imp/
2. 中文全词覆盖（Whole Word Masking）BERT的预训练模型 https://www.ctolib.com/ymcui-Chinese-BERT-wwm.html
3. 从0到1开始训练一个bert语言模型 https://blog.csdn.net/luoyexuge/article/details/85001859
4. BERT模型从训练到部署 https://blog.csdn.net/xmxoxo/article/details/89315370
