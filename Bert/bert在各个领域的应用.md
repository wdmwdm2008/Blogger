### 问题一. 在更多领域、更大规模的数据情况下，是否真的像 Bert 原始论文里的实验展示的那样，预训练技术对于很多应用领域有了很大的促进作用？如果有，作用有多大？这种作用的大小与领域相关吗？
### 问题二：Bert 作为一项新技术，肯定还有很多不成熟或者需要改进的地方，那么，Bert 目前面临的问题是什么？后面有哪些值得改进的方向？
### 百花齐放：Bert 在 NLP 各领域的应用进展

##### 1. Question Answer (QA，问答系统) 与阅读理解
- 其实搜索引擎的未来，很可能就是 QA + 阅读理解，机器学会阅读理解，理解了每篇文章，然后对于用户的问题，直接返回答案。
- 把长的文档切割成段落或句子，然后进行inverted index 和 bert

##### 2. 搜索与信息检索（IR）
- 尽管搜索和QA两个任务都是在做query和document的匹配，但是搜索更倾向于相关性，QA更倾向于语义相似度。
- 文档长度的差异：  
QA 任务往往是要查找问题 Q 的答案，而答案很可能只是一小段语言片段，在 Passage 这个较短的范围内，一般会包含正确答案，所以 QA 任务的答案一般比较短，
或者说搜索对象比较短就可以覆盖正确答案，即 QA 任务的处理对象倾向于短文本；而对搜索任务来说，文档普遍比较长。尽管在判断文档是否与查询相关时，也许只
依赖长文档中的几个关键 Passage 或者几个关键句子，但是关键片段有可能散落在文档的不同地方。在应用 Bert 的时候，因为 Bert 对于输入长度有限制，最长输入
允许 512 个单位。于是，如何处理长文档，对于搜索来说比较重要；
- 额外的特征信息：对于QA不需额外的特征信息，对于搜索：链接分析，网页质量，用户行为数据等各种其它特征也对于最终的判断也起到重要作用
- 总结：对于passage级别，QA和搜索没有什么区别，对于长文档，把文档分割成句子，利用 Bert 判断每个句子和查询 Q 的相关性，然后累加得分最高的 Top N 句子（结论是取得分最高的 3 个句子就够了，再多性能会下降），获得文档和查询 Q 的相关性得分，这样就将长文档问题转化成了文档部分句子的得分累加的模式。实验表明相对强基准 BM25+RM3，使用 Bert 会有大约 10% 的效果提升。
###### 长文档解决思路：
考虑到搜索任务的特殊性：文档和用户查询的相关性，并不体现在文章中的所有句子中，而是集中体现在文档中的部分句子中。

如果这个事实成立，那么一种直观地解决搜索任务中长文档问题的通用思路可以如下：先通过一定方法，根据查询和文档中的句子，判断两者的相关性，即产生判断函数 Score=F (Q,S)，根据 Score 得分筛选出一个较小的句子子集合 Sub_Set (Sentences)，由这些句子来代表文档内容。

这样即可将长文有针对性地缩短。从和查询相关性的角度来说，这样做也不会损失太多信息。关键是这个 F（Q，S）函数如何定义，不同的定义方法可能产生表现不同的效果。这个 F 函数，可以被称为搜索领域文档的句子选择函数，这个函数同样可以使用不同的 DNN 模型来实现。这里是有很多文章可做的，有心的同学可以关注一下。
##### 3. 对话系统／聊天机器人（Dialog System or Chatbot）
- 论文「BERT for Joint Intent Classification and Slot Filling」即是利用 Bert 解决单轮会话的会话意图分类以及槽位填充任务的。

解决方法也很直观，输入一个会话句子，Transformer 的 [CLS] 输入位置对应高层 Transformer 位置输出句子的意图分类，这是一个典型地应用 Bert 进行文本分类的方法；

另外一方面，对于会话句中的每个单词，都当作一个序列标注问题，每个单词在 Transformer 最高层对应位置，分类输出结果，将单词标注为是哪类槽的槽值的 IOE 标记即可。这是典型的用 Bert 解决序列标注的思路。而这个方法则通过 Bert 同时做了这两件事情，这点还是挺好的。


- 对于多轮对话：论文 Comparison of Transfer-Learning Approaches for Response Selection in Multi-Turn Conversations 给出了实验结果。
效果提升幅度在 11% 到 41% 之间。

##### 4. 文本摘要

- 抽取式文本摘要
目前有个论文（Fine-tune BERT for Extractive Summarization）是做抽取式文本摘要的。
- 生成式文本摘要
思路是：符合典型的 Encoder-Decoder 技术框架，只需要用预训练好的 Bert 模型初始化 Encoder 端 Transformer 参数即可
##### 5. NLP 中的数据增强
- 论文：Conditional BERT Contextual Augmentation
- 将 Bert 的双向语言模型，改造成条件语言模型。在输入端附加了一个条件，就是这个训练数据 a 的类标号，假设训练数据的类标号已知，要求根据训练数据 a 的类标号以及上下文，通过 Bert 去预测某些单词。
- 另外一篇论文 Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering
- 如何使用这些增强的训练数据。
- 有价值的结论是：如果同时增加通过增强产生的正例和负例，有助于增加 Bert 的应用效果；而且 Stage-wise 方式增加增强数据（就是原始训练数据和增强训练数据分多个阶段依次进行训练，而且距原始训练数据越远的应该越先进行 Fine-tuning），效果好于把增强数据和原始数据混合起来单阶段训练的模式。
##### 6. 文本分类
论文：DocBERT: BERT for Document Classification

##### 7. 序列标注
论文：Toward Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning


##### 7. 其它
论文：Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence
论文：Unifying Question Answering and Text Classification via Span Extraction


### Fine-tuning 阶段的 In Domain 和 Out Domain 问题

### Bert 擅长做什么？
第一，如果 NLP 任务偏向在语言本身中就包含答案，而不特别依赖文本外的其它特征，往往应用 Bert 能够极大提升应用效果。典型的任务比如 QA 和阅读理解，正确答案更偏向对语言的理解程度，理解能力越强，解决得越好，不太依赖语言之外的一些判断因素，所以效果提升就特别明显。

反过来说，对于某些任务，除了文本类特征外，其它特征也很关键，比如搜索的用户行为／链接分析／内容质量等也非常重要，所以 Bert 的优势可能就不太容易发挥出来。再比如，推荐系统也是类似的道理，Bert 可能只能对于文本内容编码有帮助，其它的用户行为类特征，不太容易融入 Bert 中。

第二，Bert 特别适合解决句子或者段落的匹配类任务。就是说，Bert 特别适合用来解决判断句子关系类问题，这是相对单文本分类任务和序列标注等其它典型 NLP 任务来说的，很多实验结果表明了这一点。

第三，Bert 的适用场景，与 NLP 任务对深层语义特征的需求程度有关。感觉越是需要深层语义特征的任务，越适合利用 Bert 来解决；而对有些 NLP 任务来说，浅层的特征即可解决问题，典型的浅层特征性任务比如分词，POS 词性标注，NER，文本分类等任务，这种类型的任务，只需要较短的上下文，以及浅层的非语义的特征，貌似就可以较好地解决问题，所以 Bert 能够发挥作用的余地就不太大，有点杀鸡用牛刀，有力使不出来的感觉。

第四，Bert 比较适合解决输入长度不太长的 NLP 任务，而输入比较长的任务，典型的比如文档级别的任务，Bert 解决起来可能就不太好。主要原因在于：Transformer 的 self attention 机制因为要对任意两个单词做 attention 计算，所以时间复杂度是 n 平方，n 是输入的长度。
### 如何寻找未开垦的 Bert 应用领域
- 输入不太长，最好是句子或者段落，避免 Bert 长文档的问题；
- 语言本身就能够很好的解决问题，不依赖其它类型的特征；
- 非生成类任务，避开目前 Bert 做生成类任务效果不够好的雷点；
- 最好是能够涉及到多句子关系判断类任务，充分利用 Bert 善于做句子匹配任务的特点；
- 最好是能够牵扯到语义层级的任务，充分利用 Bert 能够编码深层语言知识的优点；
- 如果是单输入问题，你想想能不能加入辅助句，把它改造成句子匹配型双输入任务；
### Bert 能一统 NLP 的天下吗

### Reference
https://cloud.tencent.com/developer/article/1446190
