### 问题一. 在更多领域、更大规模的数据情况下，是否真的像 Bert 原始论文里的实验展示的那样，预训练技术对于很多应用领域有了很大的促进作用？如果有，作用有多大？这种作用的大小与领域相关吗？
### 问题二：Bert 作为一项新技术，肯定还有很多不成熟或者需要改进的地方，那么，Bert 目前面临的问题是什么？后面有哪些值得改进的方向？
### 百花齐放：Bert 在 NLP 各领域的应用进展

##### 1. Question Answer (QA，问答系统) 与阅读理解
- 其实搜索引擎的未来，很可能就是 QA + 阅读理解，机器学会阅读理解，理解了每篇文章，然后对于用户的问题，直接返回答案。
- 把长的文档切割成段落或句子，然后进行inverted index 和 bert

##### 2. 搜索与信息检索（IR）
- 尽管搜索和QA两个任务都是在做query和document的匹配，但是搜索更倾向于相关性，QA更倾向于语义相似度。
- 文档长度的差异：  
QA 任务往往是要查找问题 Q 的答案，而答案很可能只是一小段语言片段，在 Passage 这个较短的范围内，一般会包含正确答案，所以 QA 任务的答案一般比较短，
或者说搜索对象比较短就可以覆盖正确答案，即 QA 任务的处理对象倾向于短文本；而对搜索任务来说，文档普遍比较长。尽管在判断文档是否与查询相关时，也许只
依赖长文档中的几个关键 Passage 或者几个关键句子，但是关键片段有可能散落在文档的不同地方。在应用 Bert 的时候，因为 Bert 对于输入长度有限制，最长输入
允许 512 个单位。于是，如何处理长文档，对于搜索来说比较重要；
- 文档长度的差异：

##### 3. 对话系统／聊天机器人（Dialog System or Chatbot）

##### 4. 文本摘要

- 抽取式文本摘要
- 生成式文本摘要

##### 5. NLP 中的数据增强

##### 6. 文本分类

##### 7. 序列标注

##### 7. 其它

### Fine-tuning 阶段的 In Domain 和 Out Domain 问题

### Bert 擅长做什么？

### 如何寻找未开垦的 Bert 应用领域

### Bert 能一统 NLP 的天下吗

### Reference
https://cloud.tencent.com/developer/article/1446190
